Return-Path: <xdp-newbies-owner@vger.kernel.org>
X-Original-To: lists+xdp-newbies@lfdr.de
Delivered-To: lists+xdp-newbies@lfdr.de
Received: from out1.vger.email (out1.vger.email [IPv6:2620:137:e000::1:20])
	by mail.lfdr.de (Postfix) with ESMTP id 3EB6F5726CB
	for <lists+xdp-newbies@lfdr.de>; Tue, 12 Jul 2022 21:55:43 +0200 (CEST)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S231199AbiGLTzi (ORCPT <rfc822;lists+xdp-newbies@lfdr.de>);
        Tue, 12 Jul 2022 15:55:38 -0400
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:35130 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S233300AbiGLTze (ORCPT
        <rfc822;xdp-newbies@vger.kernel.org>);
        Tue, 12 Jul 2022 15:55:34 -0400
Received: from mail-lf1-x12a.google.com (mail-lf1-x12a.google.com [IPv6:2a00:1450:4864:20::12a])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id 2752D1F5
        for <xdp-newbies@vger.kernel.org>; Tue, 12 Jul 2022 12:55:33 -0700 (PDT)
Received: by mail-lf1-x12a.google.com with SMTP id u13so15721532lfn.5
        for <xdp-newbies@vger.kernel.org>; Tue, 12 Jul 2022 12:55:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20210112;
        h=mime-version:from:date:message-id:subject:to;
        bh=j9U/GdvSSJWI0eeMp6URicbgann3+dOCq/dC6oq697c=;
        b=ReqIkj8Q9UEaN+hNImrTz0dalqqTDX93EOeEsKp9OR/ryBoJcVNtZ8Z2888WgVJfeb
         G8z1eQyQelKdvwgqWUIcQslcdT2zrVrvuGLAcVHzMw0XKdJJYSTn9G7GgAIZglt/sWcy
         KqJowqlBW5pe7qLdxb0JicSsEoo0QDUrMHT2QCN7DA2+ZAsfCYNcbMkR9o9RgeIW41gy
         3Bnr8dY+xigXSuP2S0ig4PYSrDhI2QdtE2JTAQ877d0ow7HH5SK7ZbberAkRYoPzMJHB
         /KZDLLKjRWRou3OjVgCq/7PvkFxdE0jqngjpYoPcvmxRWduJ67K4OKfj0cZXhCpVX74Y
         zd8g==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to;
        bh=j9U/GdvSSJWI0eeMp6URicbgann3+dOCq/dC6oq697c=;
        b=0B9ZpW4JwNlIirvAssxqXlFGyUgiIplieqqwEGi9HvAcKNdRvIJMmPSV1XRdCEWYml
         OOI39zmfQnM8WeCnHLS9baZyKPACLP+mqJ8b+pQgDIWhkzfIkwmNXfELoRZKTaw5o2Yj
         Lb8CTH69t9EN+v3fAB0qgER6ChcMF5ZvXgMa6sOdMI7GIHvfZTvPiLCdunQqBaYJmsta
         oJn2DXVvrMpYPrJVDasgmtYorMK4mXNPMdseetlS8//FJWcXtPJnlnRFPH0inRZP4ABo
         IOmSGk1e1qY1Kuq5TG1lXfOh+vFOd+A0FeorKi9055fUdMS8OLM4SND7i+lofJw+Zt61
         ZGgQ==
X-Gm-Message-State: AJIora+KwCLb4442+nKk9wa1r3F5IN2YTHyD7HvItJ2TiJrdccIztxcH
        +CLLv9ULUtMDXxRymmE/bwMebdv5/J0u2eC+kGshhXdAeAw=
X-Google-Smtp-Source: AGRyM1vrYKgSPB+0U2m1Zgs1dEvqw5v5k5sodQJuILL5EhuKXJ4KoIWEPR5L4wO6xasQGhe1G7mYvlNRbhrYAS7G/5Y=
X-Received: by 2002:ac2:43a9:0:b0:489:dccf:417a with SMTP id
 t9-20020ac243a9000000b00489dccf417amr7690663lfl.127.1657655730297; Tue, 12
 Jul 2022 12:55:30 -0700 (PDT)
MIME-Version: 1.0
From:   Adam Smith <hrotsvit@gmail.com>
Date:   Tue, 12 Jul 2022 14:55:19 -0500
Message-ID: <CADx6jH6nFOGjM1Q3Yj65Bh4fcT_qjf-k-d31vpGsxHbD-2=g=w@mail.gmail.com>
Subject: XDP redirect throughput with multi-CPU i40e
To:     xdp-newbies@vger.kernel.org
Content-Type: text/plain; charset="UTF-8"
X-Spam-Status: No, score=-2.1 required=5.0 tests=BAYES_00,DKIM_SIGNED,
        DKIM_VALID,DKIM_VALID_AU,DKIM_VALID_EF,FREEMAIL_FROM,
        RCVD_IN_DNSWL_NONE,SPF_HELO_NONE,SPF_PASS,T_SCC_BODY_TEXT_LINE
        autolearn=ham autolearn_force=no version=3.4.6
X-Spam-Checker-Version: SpamAssassin 3.4.6 (2021-04-09) on
        lindbergh.monkeyblade.net
Precedence: bulk
List-ID: <xdp-newbies.vger.kernel.org>
X-Mailing-List: xdp-newbies@vger.kernel.org

Hello,

I have a question regarding bpf_redirect/bpf_redirect_map and latency
that we are seeing in a test. The environment is as follows:

- Debian Bullseye, running 5.18.0-0.bpo.1-amd64 kernel from
Bullseye-backports (Also tested on 5.16)
- Intel Xeon X3430 @ 2.40GHz. 4 cores, no HT
- Intel X710-DA2 using i40e driver included with the kernel.
- Both interfaces (enp1s0f0 and enps0f1) in a simple netfilter bridge.
- Ring parameters for rx/tx are both set to the max of 4096, with no
other nic-specific parameters changed.

Each interface has 4 combined IRQs, pinned per set_irq_affinity.
`irqbalanced` is not installed.

Traffic is generated by another directly attached machine via iperf3
3.9 (`iperf3 -c -t 0 192.168.1.3 --bidir`) to a directly attached
server on the other side.

The server in question does nothing more than forward packets as a
transparent bridge.

An XDP program is installed on f0 to redirect to f1, and f1 to
redirect to f0. I have tried programs that simply call
`bpf_redirect()`, as well as programs that share a device map and call
`bpf_redirect_map()`, with idententical results.

When channel parameters for each interface are reduced to a single IRQ
via `ethtool -L enp1s0f0 combined 1`, and both interface IRQs are
bound to the same CPU core via smp_affinity, XDP produces improved
bitrate with reduced CPU utilization over non-XDP tests:
- Stock netfilter bridge: 9.11 Gbps in both directions at 98%
utilization of pinned core.
- XDP: Approximately 9.18 Gbps in both directions at 50% utilization
of pinned core.

However, when multiple cores are engaged (combined 4, with
set_irq_affinity), XDP processes markedly fewer packets per second
(950,000 vs approximately 1.6 million). iperf3 also shows a large
number of retransmissions in its output regardless of CPU engagement
(approximately 6,500 with XDP over 2 minutes vs 850 with single core
tests).

This is a sample taken from linux/samples xdp_monitor showing
redirection and transmission of packets with XDP engaged:

Summary                              944,508 redir/s            0
err,drop/s    944,506 xmit/s
  kthread                                           0 pkt/s
   0 drop/s                   0 sched
  redirect total                        944,508 redir/s
      cpu:0                               470,148 redir/s
      cpu:2                                 15,078 redir/s
      cpu:3                               459,282 redir/s
  redirect_err                                    0 error/s
  xdp_exception                                0 hit/s
  devmap_xmit total               944,506 xmit/s               0
drop/s         0 drv_err/s
     cpu:0                                 470,148 xmit/s
 0 drop/s         0 drv_err/s
     cpu:2                                   15,078 xmit/s
  0 drop/s         0 drv_err/s
     cpu:3                                 459,280 xmit/s
 0 drop/s         0 drv_err/s
  xmit enp1s0f0->enp1s0f1    485,249 xmit/s                0 drop/s
     0 drv_err/s
     cpu:0                                 470,172 xmit/s
  0 drop/s         0 drv_err/s
     cpu:2                                   15,078 xmit/s
   0 drop/s         0 drv_err/s
  xmit enp1s0f1->enp1s0f0    459,263 xmit/s                0 drop/s
     0 drv_err/s
     cpu:3                                 459,263 xmit/s
  0 drop/s         0 drv_err/s

Our current hypothesis is that this is a CPU affinity issue. We
believe a different core is being used for transmission. In efforts to
prove this, how can we successfully measure if bpf_redirect() is
causing packets to be transmitted by a different core than they were
received by? We are still trying to understand how bpf_redirect()
selects which core/IRQ to transmit on and would appreciate any insight
or followup material to research.

Any additional information on how we might be able to overcome this
would be deeply appreciated!

Best regards,
Adam Smith
